{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCreates Word2Vec Model, and LDA Model\\ninput: raw unstructured EMR data (['text'] column in our schema)\\noutput:Word2Vec model (.bin) using sentences, and phrases; LDA model\\nLast update: 1.31.20\\nAuthor:  Andrew Malinow, PhD\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates Word2Vec Model, and LDA Model\n",
    "input: raw unstructured EMR data (['text'] column in our schema)\n",
    "output:Word2Vec model (.bin) using sentences, and phrases; LDA model\n",
    "Last update: 1.31.20\n",
    "Author:  Andrew Malinow, PhD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import re\n",
    "import requests\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get data\n",
    "\"\"\"\n",
    "start_time=time.time()\n",
    "json_count=requests.get('http://10.32.22.16:56733/noteeventscount').json()\n",
    "count = json_count['note_count']\n",
    "page_count = math.ceil(count/100000)\n",
    "all_notes = []\n",
    "for i in range(page_count):\n",
    "    resp = requests.get('http://10.32.22.16:56733/noteevents/page/'+str(i+1))\n",
    "    notes = resp.json()['json_notes']\n",
    "    all_notes += notes\n",
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to retrieve all notes: -1413.8216257095337\n"
     ]
    }
   ],
   "source": [
    "print ('time to retrieve all notes:', (end_time-stgart_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "notes_text=sent_tokenize(str(all_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19476657\n"
     ]
    }
   ],
   "source": [
    "print (len(notes_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences=notes_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19476657\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "validate data\n",
    "(there should be more sentences than number of records)\n",
    "\"\"\"\n",
    "print (len(new_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate n-grams function\n",
    "\"\"\"\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence into tokens, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    tokens = [token for token in s.split(\" \") if len(token)>=3]\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate n-grams for new_sentences to prep for topic modeling\n",
    "\"\"\"\n",
    "ngrams=[]\n",
    "for i in new_sentences:\n",
    "    n=generate_ngrams(str(i),5)\n",
    "    ngrams.append(n)\n",
    "\n",
    "new_sentences_ngrams=ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "turn ngrams into single 'words' by replacing \" \" with \"_\"\n",
    "\"\"\"\n",
    "sentences_ngrams_concat=[]\n",
    "for i in new_sentences_ngrams:\n",
    "    ngram_list=[]\n",
    "    ngrams=new_sentences_ngrams\n",
    "    for n in ngrams:\n",
    "        n=str(n)\n",
    "        a=str(n).replace(\" \",\"_\")\n",
    "        ngram_list.append(a)\n",
    "        continue\n",
    "    sentences_ngrams_concat.append(ngram_list)\n",
    "    continue\n",
    "sentences_ngrams_concat=sentences_ngrams_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pre-processing: tokenize 'sentences_ngrams_concat' for topic modeling\n",
    "\"\"\"\n",
    "ngrams_concat_tokens=[]\n",
    "for n in sentences_ngrams_concat: \n",
    "    ngrams_concat_tokens.append(word_tokenize(str(n)))\n",
    "ngram_tokens=ngrams_concat_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "feature engineering-derived: use LDA on clinical ngrams concat\n",
    "create dictionary and corpus and save for future use\n",
    "\"\"\"\n",
    "start=time.time()\n",
    "dictionary = gensim.corpora.Dictionary(sentences_ngrams_concat)\n",
    "\n",
    "#create corpus \n",
    "corpus = [dictionary.doc2bow(text) for text in sentences_ngrams_concat]\n",
    "#save corpus and dictionary\n",
    "pickle.dump(corpus, open('Default_n_grams-corpus.pkl', 'wb'))\n",
    "dictionary.save('Default_Dictionary')\n",
    "end=time.time()\n",
    "print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create Word2Vec Model and save for future use\n",
    "need to update location for saved model\n",
    "\"\"\"\n",
    "\n",
    "model = Word2Vec(ngram_concat_tokens, size=100, window=10, min_count=1, workers=3)\n",
    "model.wv.save_word2vec_format('Word2VecModelSentences.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use topic modeling to extract themes\n",
    "train and save an LDA model\n",
    "use num_topics parameter to determine the number of topics for the model,\n",
    "and num_words parameter for how much to show\n",
    "\"\"\"\n",
    "lda=gensim.models.LdaMulticore(corpus=corpus,num_topics=5,id2word=dictionary,passes=10,workers=3)\n",
    "lda.save(\"mimic-lda-full_notes.model\")\n",
    "topics=lda.print_topics(num_words=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in topics:\n",
    "    print (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.wv.most_similar('heart'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global Variables\n",
    "\"\"\"\n",
    "model_file='Word2VecModel.bin'\n",
    "model=model = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True,unicode_errors='ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
